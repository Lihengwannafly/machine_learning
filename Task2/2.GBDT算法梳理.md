### 前言

提升(Boosting)方法是集成学习方法的一种，在分类问题中，它通过改变训练样本的权重，学习多个分类器，将弱分类器组装成一个强分类器，提高分类的性能。其最具代表性的是AdaBoost算法。

提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。提升树(Boosting Tree)是以决策树为基函数的提升方法。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。

提升树模型可以表示为决策树的加法模型：
$$
f_M(x)=\sum ^M_{i=1}T(x;\theta_m)
$$

其中，$M​$表示决策树的个数，$T(x;\theta_m)​$表示决策树，$\theta_m​$表示第$m​$个决策树参数。

不同问题的提升树学习算法，其主要区别在于损失函数不同。

1. 平方损失函数常用于回归问题，
2. 指数损失函数用于分类问题，
3. 一般损失函数用一般于决策问题。

GBDT有很多简称，有GBT（Gradient Boosting Tree）、GTB(Gradient Tree Boosting )、GBRT(Gradient Boosting Regression Tree)、 MART(Multiple Additive Regression Tree)，这些其实都是一种算法。

GBDT也是使用的前向分步算法和加法模型。

### 1.前向分步算法

加法模型(Additive Model)
$$
f(x)=\sum^M_{m=1}\beta_mb(x;\gamma_m)
$$


其中，$b(x;\gamma_m)$为基函数，$\beta_m$为基函数的系数。本质上来讲，加法模型就是基函数的线性组合。

在给定训练数据和损失函数$L(y,f(x))$的条件下，学习加法模型成为损失函数极小化问题：
$$
min_{\beta_m, \gamma_m}\sum^N_{i=1}L(y_i,\sum^M_{m=1}\beta_mb(x_i;\gamma_m))
$$
前向分步算法求解这一优化问题的思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式：
$$
min_{\beta, \gamma}\sum^N_{i=1}L(y_i,\beta b(x_i;\gamma))
$$
前向分步算法流程：

输入：训练集$T=\{(x_1,y_), (x_2,y_2),...,(x_N,y_N)\}​$，损失函数$L(y,f(x))​$，基函数集$\{b(x;\gamma)\}​$；

输出：加法模型$f(x)​$。

求解步骤如下：

1. 初始化$f_0(x)=0$

2. 对于$m=1,2...,M$

   1. 极小化损失函数

      $(\beta_m,\gamma_m)=\arg \min_{\beta, \gamma} \sum^N_{i=1}L(y_i, f_{m-1}(x_i)+\beta b(x_i;\gamma))$

      得到参数$\beta_m, \gamma_m$

   2. 更新
      $$
      f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)
      $$
      

3. 得到加法模型
   $$
   f(x)=f_M(x)=\sum^M_{m=1}\beta_m b(x;\beta_m)
   $$

由此可见，前向分步算法将同时求解从$m=1 \rightarrow  M$所有参数$\beta_m, \gamma_m$的优化问题简化为逐步求解各个$\beta_m, \gamma_m$的优化问题。

### 2.负梯度拟合

#### 2.1提升树算法

提升方法实际采用加法模型与前向分步算法，以决策树作为基函数的提升方法称为提升树。注意，这里的决策树为**CART回归树**，不是分类树。当问题是分类问题时，采用的决策树模型为分类回归树。为什么要采用决策树作为基函数呢？它有以下优缺点：

优点：

- 可解释性强
- 可处理混合类型特征
- 具有伸缩不变性（不用归一化特征）
- 有特征组合的作用
- 可自然处理缺失值
- 对异常点鲁棒
- 有特征选择作用
- 可扩展性强，容易并行

缺点：

- 缺乏平滑性（回归预测时输出值只能是输出有限的若干种数值）
- 不适合处理高维稀疏数据

由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此。

提升树模型可表示为：
$$
f_M(x)=\sum^M_{m=1}T(x;\theta_m)
$$
其中，$M$表示决策树的个数，$T(x;\theta_m)$表示决策树，$\theta_m$表示第$m$个决策树参数。

针对不同的问题，提升树算法的形式有所不同，其主要区别在于使用的损失函数不同。而损失函数的不同，决策树要拟合的值也会不同。就一般而言，对于回归问题的提升树算法来说，若损失函数是平方损失函数，每一步只需简单拟合当前模型的残差。

下面看一下在回归问题下，损失函数为平方损失函数的算法流程：

输入：训练集$T=\{(x_1,y_), (x_2,y_2),...,(x_N,y_N)\}​$，损失函数$L(y,f(x))​$，基函数集$\{b(x;\gamma)\}​$；

输出：提升树模型$f_M(x)$。

求解步骤如下：

1. 初始化$f_0(x)=0$

2. 对于$m=1,2...,M$

   1. 计算残差

      $r_{mi}=y_i-f_{m-1}(x_i), i=1,2,...,N$

   2. 拟合残差$r_{mi}$,学习一个回归树，得到$T(x;\theta_m)$

   3. 更新
      $$
      f_m(x)=f_{m-1}(x)+T(x;\theta_m)
      $$

3. 得到回归树问题模型
   $$
   f_M(x)=\sum^M_{m=1}T(x;\theta_m)
   $$



#### 2.2 梯度提升

提升树用加法模型与前向分布算法实现学习的优化过程。当损失函数为平方损失和指数损失函数时，每一步优化是很简单的。但对于一般损失函数而言，往往每一步都不那么容易。对于这问题，Freidman提出了梯度提升算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值：
$$
\left . -\frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right | _{f(x)=f_{m-1}(x)}
$$
对于分类问题和回归问题，通过其损失函数的负梯度的拟合，就可以用 GBDT 来解决分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。

### 3.损失函数

在GBDT算法中，损失函数的选择十分重要。针对不同的问题，损失函数有不同的选择。

对于分类算法，其损失函数一般由对数损失函数和指数损失函数两种。

1. 指数损失函数表达式：

   $L(y,f(x))=e^{-yf(x)}$

2. 对数损失函数可分为二分类和多分类两种。

   $L(y,f(x))=\log(1+\exp(-yf(x))$

   $L(y, f(x))=-\sum_{k=1}^{K} y_{k} \log \left(p_{k}(x)\right)$

对于回归算法，常用损失函数有如下4种。

1. 平方损失函数：

   $L(y,f(x))=(y-f(x))^2$

2. 绝对损失函数：

   $L(y,f(x))=\left |  y-f(x) \right |  ​$，

   对应负梯度误差为：

   $sign(y_i-f(x_i))$

3. Huber损失，它是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失误差，而对于靠近中心的点则采用平方损失。这个界限一般用分位数点度量。损失函数如下：
   $$
   L(y, f(x))=\left\{\begin{array}{ll}{\frac{1}{2}(y-f(x))^{2}} & {|y-f(x)| \leq \delta} \\ {\delta\left(|y-f(x)|-\frac{\delta}{2}\right)} & {|y-f(x)|>\delta}\end{array}\right.
   $$
   对应的负梯度误差为：
   $$
   r\left(y_{i}, f\left(x_{i}\right)\right)=\left\{\begin{array}{ll}{y_{i}-f\left(x_{i}\right)} & {\left|y_{i}-f\left(x_{i}\right)\right| \leq \delta} \\ {\delta \operatorname{sign} \left(y_{i}-f\left(x_{i}\right)\right)} & {\left|y_{i}-f\left(x_{i}\right)\right|>\delta}\end{array}\right.
   $$
   

4. 分位数损。它对应的是分位数回归的损失函数，表达式为：
   $$
   L(y, f(x))=\sum_{y \geq f(x)} \theta|y-f(x)|+\sum_{y<f(x)}(1-\theta)|y-f(x)|
   $$
   其中$\theta$为分位数，需要我们在回归之前指定。对应的负梯度误差为：
   $$
   r\left(y_{i}, f\left(x_{i}\right)\right)=\left\{\begin{array}{ll}{\theta} & {y_{i} \geq f\left(x_{i}\right)} \\ {\theta-1} & {y_{i} \leq f\left(x_{i}\right)}\end{array}\right.
   $$
   对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。

### 4.回归问题

输入：训练集$T=\{(x_1,y_), (x_2,y_2),...,(x_N,y_N)\}$，损失函数$L(y,f(x))$，基函数集$\{b(x;\gamma)\}$；

输出：回归树模型$\widetilde{f}(x)​$。

求解步骤如下：

1. 初始化$f_0(x)=\arg \min_c \sum^N_{i=1}L(y_i,c)$

   注：估计使损失函数极小化的常数值，它是只有一个根结点的树

2. 对于$m=1,2...,M​$

   1. 计算残差

      $r_{mi}=-\frac{\partial L(y,f(x_i))}{\partial f(x_i)}| {f(x)=f_{m-1}(x)}, i=1,2,...,N$

      注： 计算损失函数在当前模型的值，作为残差的估计

   2. 拟合残差$r_{mi}$,学习一个回归树，得到第m棵树的叶节点区域$R_{mj}, j=1,2,...,J$

   3. 对于$j=1,2,...,J$，计算

      $c_{mj}=\arg \min _c \sum_{x_i\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$

      注：在损失函数极小化条件下，估计出相应叶结点区域的值

   4. 更新
      $$
      f_m(x)=f_{m-1}(x)+\sum^J_{j=1}c_{mj}I(x\in R_{mj})
      $$

3. 得到回归树问题模型
   $$
   \widetilde{f}(x)=f_M(x)=\sum^M_{m=1}\sum^J_{j=1}c_{mj}I(x\in R_{mj})
   $$


### 5.分类问题

GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合输出类别的误差。

为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法用类似逻辑回归的对数似然函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。此处仅讨论用对数似然函数的GBDT分类。对于对数似然损失函数，我们有又有二元分类和的多元分类的区别。

#### 5.1 二分类

对于二分类GBDT，如果用类似逻辑回归的对数似然损失函数，则损失函数为：
$$
L(y,f(x))=\log(1+\exp(-yf(x))
$$
其中，$y \in \{ -1, 1\}$，此时的负梯度误差为：
$$
r_{ti}=\left . -\frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right | _{f(x)=f_{t-1}(x)} = \frac{y_i}{1+\exp(y_if(x_i))}
$$
对于生产的决策树，我们各个叶子节点的最佳负梯度拟合值为：
$$
c_{tj}=\arg \min _c \sum _{x_i \in R_{tj}} \log (1+\exp(-y_i(f_{t-1}(x_i)+c)))
$$
由于上式比较难优化，我们一般使用近似值代替：
$$
c_{t j}=\frac{\sum_{x_{i} \in R_{t j}} r_{t j}}{\sum_{x_{i} \in R_{t j}}\left|r_{t j}\right|\left(1-\left|r_{t j}\right|\right)}
$$
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二分类GBDT与GBDT回归算法过程相同。

#### 5.2 多分类

多分类GBDT比二分类GBDT复杂些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：
$$
L(y, f(x))=-\sum_{k=1}^{K} y_{k} \log \left(p_{k}(x)\right)
$$
其中，如果样本输出类别为$k$，则$y_k=1$。第$k$类的概率$p_k(x)$的表达式：
$$
p_{k}(x)=\frac{\exp \left(f_{k}(x)\right)}{\sum_{l=1}^{K} \exp \left(f_{l}(x)\right)}
$$
集合上两式，我们可以计算出第$t$轮的第$i$个样本对应类别$l$的负梯度误差为：
$$
t_{t i l}=-\left .\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right|_{f_{k}(x)=f_{l, t-1}(x)}=y_{i l}-p_{l, t-1}\left(x_{i}\right)
$$
观察上式可以看出，其实这里的误差就是样本$i$对应类别$l$的真实概率和$t-1$轮预测概率的差值。

对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为：
$$
c_{t j l}=\operatorname{argmin}_{c_{j l}} \sum_{i=0}^{m} \sum_{k=1}^{K} L\left(y_{k}, f_{t-1, l}(x)+\sum_{j=0}^{J} c_{j l} I\left(x_{i} \in R_{t j}\right)\right)
$$
由于上式比较难优化，我们一般使用近似值代替：
$$
c_{t j l}=\frac{K-1}{K} \frac{\sum_{x_{i} \in R_{t j}} r_{t i l}}{\sum_{x_{i} \in R_{t i l}}\left|r_{t i l}\right|\left(1-\left|r_{t i l}\right|\right)}
$$
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多分类GBDT与二分类GBDT以及GBDT回归算法过程相同。

### 6.正则化

对GBDT进行正则化来防止过拟合，主要有三种形式。

1. 给每棵数的输出结果乘上一个步长$a$（learning rate）

   对于前面的弱学习器的迭代：
   $$
   f_{m}(x)=f_{m-1}(x)+T\left(x ; \gamma_{m}\right)
   $$
   加上正则化项，则有
   $$
   f_{m}(x)=f_{m-1}(x)+a T\left(x ; \gamma_{m}\right)
   $$
   此处，$a$的取值范围为$(0,1]$。对于同样的训练集学习效果，较小的$a$意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起决定算法的拟合效果。

2. 第二种正则化的方式就是通过**子采样比例(subsample)**。取值范围为$(0,1]$。

   GBDT这里的做法是在每一轮建树时，样本是从原始训练集中采用无放回随机抽样的方式产生，与随机森立的有放回抽样产生采样集的方式不同。若取值为1，则采用全部样本进行训练，若取值小于1，则不选取全部样本进行训练。选择小于1的比例可以减少方差，防止过拟合，但可能会增加样本拟合的偏差。

3. 第三种是对弱学习器即CART回归树进行正则化剪枝。（如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等）

### 7.GBDT优缺点

#### 7.1 GBDT优点

- 可以灵活处理各种类型的数据，包括连续值和离散值。
- 在相对较少的调参时间情况下，预测的准确率也比较高，相对SVM而言。
- 在使用一些健壮的损失函数，对异常值得鲁棒性非常强。比如Huber损失函数和Quantile损失函数。

#### 7.2 GBDT缺点

- 由于弱学习器之间存在较强依赖关系，难以并行训练。可以通过自采样的SGBT来达到部分并行。

### 8.sklearn参数

在scikit-learning中，GradientBoostingClassifier对应GBDT的分类算法，GradientBoostingRegressor对应GBDT的回归算法。

具体算法参数情况如下：

```
GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, 
                subsample=1.0, criterion=’friedman_mse’, min_samples_split=2,
                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,
                min_impurity_decrease=0.0, min_impurity_split=None, init=None, 
                random_state=None, max_features=None, alpha=0.9, verbose=0, 
                max_leaf_nodes=None, warm_start=False, presort=’auto’, 
                validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)
```

**参数说明：**

- n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。
- learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。
- subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。
- init：我们初始化的时候的弱学习器。若不设置，则使用默认的。
- loss：损失函数，可选{'ls'-平方损失函数，'lad'绝对损失函数-,'huber'-huber损失函数,'quantile'-分位数损失函数}，默认'ls'。
- alpha：当我们在使用Huber损失"Huber"和分位数损失"quantile"时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。
- criterion：决策树节搜索最优分割点的准则，默认是"friedman_mse"，可选"mse"-均方误差与'mae"-绝对误差。
- max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。
- max_depth：树的最大深度。
- min_samples_split：内部节点再划分所需最小样本数。
- min_samples_leaf：叶子节点最少样本数。
- max_leaf_nodes：最大叶子节点数。
- min_impurity_split：节点划分最小不纯度。
- presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。
- validation*fraction：*为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。
- n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。

### 8.GBDT应用场景

GBDT几乎可以用于所有回归问题（线性/非线性），相对loigstic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于分类问题。
### 1.LightGBM

本文介绍LightGBM，它是一款常用的GBDT工具包，由微软亚洲研究院（MSRA）进行开发。其速度比XGBoost快，并且精度也相当的不错。

注意其设计理念：

> 1. 单个机器在不牺牲速度的情况下，尽可能多地用上更多的数据；
> 2. 多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速。

### 2.LightGBM的起源

在大样本和高维度的环境下，传统的boosting算法对每一个特征都要扫描所有的样本点来选择最好的切分点非常耗时。为了解决耗时的问题，LightGBM使用了如下两种解决办法：一是GOSS（Gradient-based One-Side Sampling, 基于梯度的单边采样），不是使用所用的样本点来计算梯度，而是对样本进行采样来计算梯度；二是EFB（Exclusive Feature Bundling， 互斥特征捆绑） ，这里不是使用所有的特征来进行扫描获得最佳的切分点，而是将某些特征进行捆绑在一起来降低特征的维度，使寻找最佳切分点的消耗减少。

### 3.Histogram VS Exact greedy

回顾一下XGBoost中的Exact greedy算法：

1. 对每个特征都按照特征值进行排序
2. 在每个排好序的特征都寻找最优切分点
3. 用最优切分点进行切分

这个算法比较精确，但是缺点明显：

1. 空间消耗大。需要保存数据的特征值。XGBoost采用Block结构，存储指向样本的索引，需要消耗两倍的内存。
2. 时间开销大。在寻找最优切分点时，要对每个特征都进行排序，还要对每个特征的每个值都进行了遍历，并计算增益。
3. 对Cache不友好。使用Block块预排序后，特征对梯度的访问是按照索引来获取的，是一种随机访问，而不同特征访问顺序也不一样，容易照成命中率低的问题。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的Cachemiss。

使用直方图算法进行划分点的查找可以很好的克服这些缺点。

直方图算法(Histogram algorithm)的做法是把连续的浮点特征值离散化为$k$个整数（其实又是分桶的思想，而这些桶称为bin）比如$[0,0.1) \rightarrow 0,[0.1,0.3) \rightarrow 1$。

同时，将特征根据其所在的bin进行**梯度累加**。这样，遍历一次数据后，直方图累积了需要的梯度信息，然后可以直接根据直方图，寻找最优的切分点。

![img](https://www.hrwhisper.me/wp-content/uploads/2018/07/LightGBM-histogram.png)

Histogram优点：

- 可以减少内存占用，比如离散为256个Bin时，只需要用8位整形就可以保存一个样本被映射为哪个Bin(这个bin可以说就是转换后的特征)，对比预排序的Exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。
- 计算效率也得到提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益，复杂度为O(#feature×#data)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为O(#feature×#bins)。
- 提高缓存命中率，因为它访问梯度是连续的（直方图）。
- 此外，在数据并行的时候，直方图算法可以**大幅降低通信代价。**（数据并行、特征并行在本文后面讲解）

Histogram缺点：

histogram 算法不能找到很精确的分割点。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点**也有正则化的效果**，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。

### 4.leaf-wise VS level-wise

在XGBoost中，树是按层生长的，称为**Level**-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：

![img](https://www.hrwhisper.me/wp-content/uploads/2018/07/LightGBM-level-wise-tree-growth.png)

Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

而LightGBM采用的是**Leaf**-wise tree growth：

![img](https://www.hrwhisper.me/wp-content/uploads/2018/07/LightGBM-leaf-wise-tree-growth.png)

Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

### 5.特征并行和数据并行

### 特征并行

特征并行主要是并行化决策树中寻找最优划分点(“Find Best Split”)的过程，因为这部分最为耗时。

#### 传统算法

传统算法的做法如下：

1. 垂直划分数据（**对特征划分**），不同的worker有**不同的特征集**
2. 每个workers找到局部最佳的切分点{feature, threshold}
3. workers使用点对点通信，找到全局最佳切分点
4. 具有全局最佳切分点的worker进行节点分裂，然后广播切分后的结果（左右子树的instance indices）
5. 其它worker根据收到的instance indices也进行划分

![img](https://www.hrwhisper.me/wp-content/uploads/2018/07/LightGBM-feature-parallelization.png)

传统算法的缺点是：

1. 无法加速split的过程，该过程复杂度为O(#data)，当数据量大的时候效率不高
2. 需要广播划分的结果（左右子树的instance indices），1条数据1bit的话，大约需要花费O(#data/8)

#### LightGBM中的特征并行

每个worker**保存所有的数据集**，这样找到全局最佳切分点后各个worker都可以自行划分，就不用进行广播划分结果，减小了网络通信量。过程如下：

1. 每个workers找到局部最佳的切分点{feature, threshold}
2. workers使用点对点通信，找到全局最佳切分点
3. 每个worker根据全局全局最佳切分点进行节点分裂

但是这样仍然有缺点：

1. split过程的复杂度仍是O(#data)，当数据量大的时候效率不高
2. 每个worker保存所有数据，存储代价高

### 数据并行

#### 传统算法

数据并行目标是并行化整个决策学习的过程：

1. 水平切分数据，不同的worker拥有部分数据
2. 每个worker根据本地数据构建局部直方图
3. 合并所有的局部直方图得到全部直方图
4. 根据全局直方图找到最优切分点并进行分裂

![img](https://www.hrwhisper.me/wp-content/uploads/2018/07/LightGBM-data-parallelization.png)

在第3步中，有两种合并的方式：

- 采用点对点方式(point-to-point communication algorithm)进行通讯，每个worker通讯量为O(#machine∗#feature∗#bin)
- 采用collective communication algorithm(如“[All Reduce](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html)”)进行通讯（相当于有一个中心节点，通讯后在返回结果），每个worker的通讯量为O(2∗#feature∗#bin)

可以看出通信的代价是很高的，这也是数据并行的缺点。

#### LightGBM中的数据并行

1. 使用“Reduce Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。
2. 前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。

通过上述两点做法，通信开销降为O(0.5∗#feature∗#bin)

### 6.顺序访问梯度

![img](https://fuhailin.github.io/LightGBM/20181126170158451.png)

Pre-sorted 的算法当中，有两个操作频繁的地方，会造成cache-miss。

- 对梯度的访问，在计算gain的时候，需要利用梯度，但不同的feature访问梯度顺序都是不一样的，而且是随机的。
- 对于索引表的访问，pre-sorted使用一个行号和叶子节点号的索引表，防止数据切分的时候，对所有的feature进行切分。对访问梯度一样，所有的feature都要通过访问这个索引表，所以，都是随机的访问，这个时候，会带了非常大的系统性能的下降。

![img](https://fuhailin.github.io/LightGBM/20181126170217435.png)

LightGBM使用直方图算法则是天然的cache friendly，首先，对梯度的访问，因为不需要对feature进行排序，同时，所有的feature都采用同样的方式进行访问，所以只需要对梯度访问的顺序进行一个重新的排序，所有的feature都能连续地访问梯度。

此外，直方图算法不需要数据id到叶子id的一个索引表，没有这样一个cache-miss的问题。事实上，在cache-miss这样一个方面，对速度的影响是很大的，尤其在数据量很大的时候，MRSA研究人员进行过测试，在数据量很多的时候，相比于随机访问，顺序访问的速度可以快4倍以上，这其中速度的差异基本上就是由cache-miss而带来的。

### 7.支持类别特征

实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化one-hot特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。

one-hot编码是处理类别特征的一个通用方法，然而在树模型中，这可能并不一定是一个好的方法，尤其当类别特征中类别个数很多的情况下。主要的问题是：

- 可能无法在这个类别特征上进行切分（即浪费了这个特征）。使用one-hot编码的话，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。当类别值很多时，每个类别上的数据可能会比较少，这时候切分会产生不平衡，这意味着切分增益也会很小（比较直观的理解是，不平衡的切分和不切分没有区别）。
- 会影响决策树的学习。因为就算可以在这个类别特征进行切分，也会把数据切分到很多零碎的小空间上，如图1左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习会变差。但如果使用下图右边的分裂方式，数据会被切分到两个比较大的空间，进一步的学习也会更好。

**LightGBM处理分类特征大致流程：**

为了解决one-hot编码处理类别特征的不足。LightGBM采用了Many vs many的切分方式，实现了类别特征的最优切分。用LightGBM可以直接输入类别特征，并产生上图右边的效果。在1个k维的类别特征中寻找最优切分，朴素的枚举算法的复杂度是$O\left(2^{k}\right)$，而LightGBM采用了如[On Grouping For Maximum Homogeneity](http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf)的方法实现了$O(k \log k)$的算法。

算法流程下图所示：在枚举分割点之前，先把直方图按每个类别的均值进行排序；然后按照均值的结果依次枚举最优分割点。从下图可以看到，Sum(y)/Count(y)为类别的均值。当然，这个方法很容易过拟合，所以在LGBM中加入了很多对这个方法的约束和正则化。

![img](https://www.biaodianfu.com/wp-content/uploads/2019/03/category.jpg)


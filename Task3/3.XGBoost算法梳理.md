### 前言

XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器，所用到的树模型则是CART回归树模型。讲解其原理前，先讲解一下CART回归树。

**CART回归树**：

CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第$j$个特征值进行分裂的，设该特征值小于$s$的样本划分为左子树，大于$s$的样本划分为右子树。
$$
R_{1}(j, s)=\left\{x | x^{(j)} \leq s\right\} \text { and } R_{2}(j, s)=\left\{x | x^{(j)}>s\right\}
$$
而CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题。因此，在决策树模型中是使用启发式方法解决。典型CART回归树产生的目标函数为：
$$
\mathcal{L}=\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
$$
因此，当我们为了求解最优的切分特征$j$和最优的切分点$s$，就转化为求解这么一个目标函数：
$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$
所以我们只要遍历所有特征的的所有切分点，就能找到最优的切分特征和切分点。最终得到一棵回归树。

### 1.XGBoost算法原理

XGBoost算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到$k$棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。
$$
\begin{array}{c}{\hat{y}=\phi\left(x_{i}\right)=\sum_{k=1}^{M} f_{k}\left(x_{i}\right)} \\ {\text { where } F=\left\{f(x)=w_{q(x)}\right\}\left(q : R^{m} \rightarrow T, w \in R_{ \| n}^{T}\right)}\end{array}
$$
注：$w_q(x)$为叶子节点$q$的分数，$f(x)$为其中一棵回归树 

如下图所示，训练出了2棵决策树，小孩的预测分数就是两棵树中小孩所落到的结点的分数相加。爷爷的预测分数同理。

![img](http://img.blog.itpub.net/blog/2018/08/09/3b293ae38839d5ae.jpeg?x-oss-process=style/bb)

### 2.XGBoost损失函数

XGBoost目标函数定义为：
$$
\mathcal{L}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k=1}^{K} \Omega\left(f_{k}\right)
$$
目标函数由两部分构成，第一部分$\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)$用来衡量预测分数和真实分数的差距，另一部分$\sum_{i=1}^{K} \Omega\left(f_{k}\right)$

则是正则化项。正则化项同样包含两部分，$\Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}$。$T$表示叶子结点的个数，$w$表示叶子节点的分数。$\gamma$可以控制叶子结点的个数，$\lambda$可以制叶子节点的分数不会过大，防止过拟合。 

正如上文所说，新生成的树是要拟合上次预测的残差的，即当生成$t$棵树后，预测分数可以写成： 
$$
\hat{y}_{i}^{(t)}=\hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)
$$
同时，可以将目标函数改写成：
$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)
$$
很明显，我们接下来就是要去找到一个$f_t$能够最小化目标函数。XGBoost的想法是利用其在$f_t=0$处的泰勒二阶展开近似它。
$$
f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\left(\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}\right)
$$
所以，目标函数近似为：
$$
\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)
$$
其中$g_i$为一阶导数，$h_i$为二阶导数：
$$
g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right), \quad h_{i}=\partial_{\hat{y}^{(t-1)}}^{2} l\left(y_{i}, \hat{y}^{(t-1)}\right)
$$
在损失函数中，$g_i$和$h_i$是一阶和二阶梯度统计。这就是XGboost的特点：通过这种近似，可以自行定义一些损失函数（例如，平方损失，逻辑损失），只要保证二阶可导。由于前$t-1$棵树的预测分数与$y$的残差对目标函数优化不影响，可以直接去掉。简化目标函数为：
$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)
$$
上式是将每个样本的损失函数值加起来，我们知道，每个样本都最终会落到一个叶子结点中，所以我们可以将所以同一个叶子结点的样本重组起来，过程如下图：
$$
\begin{aligned} \mathcal{L}^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\lambda \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned}
$$
因此通过上式的改写，我们可以将目标函数改写成关于叶子结点分数$w$的一个一元二次函数，求解最优的$w$和目标函数值就变得很简单了。

对于一个固定的结构$q(x)$，我们可以计算叶节点$j$的最优权重$w_{j}^{*}$：
$$
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
$$
并且计算对应的最优损失函数：
$$
\mathcal{L}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
$$
因此，最优的$w$和目标函数公式为：
$$
w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda} \\ \quad \mathcal{L}=-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T_{B}
$$

### 3.**分裂结点算法**

在上面的推导中，我们知道了如果我们一棵树的结构确定了，如何求得每个叶子结点的分数。但我们还没介绍如何确定树结构，即每次特征分裂怎么寻找最佳特征，怎么寻找最佳分裂点。

正如上文说到，基于空间切分去构造一颗决策树是一个NP难问题，我们不可能去遍历所有树结构，因此，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用上式目标函数值作为评价函数。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。

假设$I_{L}$和$I_{R}$是拆分后左右节点的实例集，则$I=I_{L} \cup I_{R}$。拆分之后的损失减少为：
$$
\mathcal{L}_{\text {split}}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
$$
这个公式通常被用于评估分割候选集（选择最优分割点），其中前两项分别是切分后左右子树的的分支之和，第三项是未切分前该父节点的分数值，最后一项是引入额外的叶节点导致的复杂度。

同时可以设置树的最大深度、当样本权重和小于设定阈值时停止生长去防止过拟合。

### 4.正则化

$$
\Omega(f)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}
$$

 **注意：**这里出现了$\gamma$和$\lambda$，这是xgboost自己定义的，在使用xgboost时，你可以设定它们的值，显然，$\lambda$越大，表示越希望获得结构简单的树，因为此时对较多叶子节点的树的惩罚越大。$\lambda$越大也是越希望获得结构简单的树。

### 5.对缺失值处理

当样本的第$i$个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。

![img](https://img2018.cnblogs.com/blog/1008048/201903/1008048-20190305004525486-405291703.png)

### 6.**XGBoost的优点**

之所以XGBoost可以成为机器学习的大杀器，广泛用于数据科学竞赛和工业界，是因为它有许多优点：

1. 使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。

2. 目标函数优化利用了损失函数关于待求函数的二阶导数

3. 支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。

4. 添加了对稀疏数据的处理。

5. 交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。

6. 支持设置样本权重，该权重体现在一阶导数$g$和二阶导数$h$，通过调整权重可以去更加关注一些样本

缺点：

1. 算法采用贪心策略，较为耗时。

### 7.应用场景

### 8.XGBoost参数

[XGBoost Parameters]: https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters


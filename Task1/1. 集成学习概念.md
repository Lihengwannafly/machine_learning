### 1. 集成学习概念

在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。
集成学习的主要方法可归类为三大类： 堆叠（Stacking）、提升（Boosting） 和 装袋（Bagging/Bootstrap）。其中最流行的方法包括随机森林、梯度提升、AdaBoost、梯度提升决策树（GBDT）和XGBoost。

### 2. 个体学习器概念

集成学习的第一个问题就是如何得到若干个个体学习器。这里有两种选择。第一种就是所有的个体学习器都是一个种类的，或者说是同质的（homogeneous），同质集成中的个体学习器也称为“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）。比如都是决策树个体学习器，或者都是神经网络个体学习器。第二种是所有的个体学习器不全是一个种类的，或者说是异质的（heterogeneous）。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。这时个体学习器一般不称为基学习器，而称作“组件学习器”（component leaner）或直接称为个体学习器。

弱学习器（weak learner）：指泛化性能略优于随机猜测的学习器：例如在二分类问题上精度略高于50%的分类器。集成学习的直觉是结合多个个体的能力，获得远超个体的集体能力优势。这种直觉在实际上对于“弱学习器”是非常符合的。故很多集成学习的研究也都是针对弱学习器，而基学习器有时也被直接称为弱学习器。

根据个体学习器生成方式的不同，目前集成学习方法大致可分为两大类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成的序列化方法，代表算法是Boosting系列算法，例如：AdaBoost，GBDT，XGBoost，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是Bagging系列算法，例如：随机森林。

### 3. Boosting

Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习1的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，因而这些误差率高的样本在后面的弱学习器2中得到更多的重视，然后基于调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。

### 4. Bagging

Bagging的算法原理和Boosting不同，它的弱学习器之间没有依赖关系，可以并行生成。

Bagging的个体弱学习器的训练集是通过随机采样得到的。比如，通过3次的随机采样，我们就可以得到3个采样集，对于这3个采样集，我们可以分别独立的训练出3个弱学习器，再对这3个弱学习器通过集合策略来得到最终的强学习器。

对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstap sampling），即对于$m$个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集 $m$ 次，最终可以得到 $m$ 个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

> 注：Bootstrap方法是非常有用的一种统计学上的估计方法。 Bootstrap是一类非参数的Monte Carlo方法,其实质是对观测信息进行再抽样，进而对总体的分布特性进行统计推断。首先，Bootstrap通过重抽样，避免了Cross-Validation造成的样本减少问题，其次，Bootstrap也可以创造数据的随机性。Bootstrap是一种有放回的重复抽样方法，抽样策略就是简单的随机抽样。

随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。其在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。

具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有$d$个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数$k$控制了随机性的引入程度：若令 $k=d$ ，则基决策树的构建与传统决策树相同；若令$k=1$ ，则是随机选择一个属性用于划分；一般情况下，推荐值$k = log_2 d$ 。

### 5.结合策略(平均法，投票法，学习法)

假设集成中包含 $T$ 个基学习器 $h_1,h_2,...,h_T$，其中 $h_i$ 在样本$x$上的输出为 $h_i(x)$。那么对$h_i$进行结合的常见策略有以下几种：

#### 5.1 平均法

对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。

最简单的平均是算术平均，也就是说最终预测是:
$$
H(x) = \frac{1}{T}\sum^T_1h_i(x)
$$
如果每个个体学习器有一个权重$w$，则最终预测是:
$$
H(x) = \frac{1}{T}\sum^T_1 w_i h_i(x)
$$
其中 $w_i$是个体学习器$h_i$的权重， $w_i\ge 0, \sum_1^Tw_i=1$

一般而言，在个体学习器的性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。

#### 5.2 投票法

对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是$ \{c_1,c_2,...,c_K\} $，对于任意一个预测样本 $x$ ，我们的$T$ 个弱学习器的预测结果分别是 $(h_1(x),h_2(x),...,h_T(x))$ 。

最简单的投票法是相对多数投票法（plurality voting），也就是我们常说的少数服从多数，也就是 $T$个弱学习器的对样本$x$ 的预测结果中，数量最多的类别 $c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。

稍微复杂的投票法是绝对多数投票法（majority voting），也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。

更加复杂的是加权投票法（weighted voting），和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。

#### 5.3 学习法

上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。

### 6.随机森林思想

随机森林由LeoBreiman（2001）提出，从原始训练样本集$N$中有放回地重复随机抽取$k$个样本生成新的训练样本集合，然后根据自助样本集生成$k$个分类树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。

其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。

特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样本可以通过每一棵树的分类结果经统计后选择最可能的分类。

### 7.随机森林的推广

RF推广算法在实际应用中占有比较好的特性，应用比较广泛，主要应用在：分类、回归、特征转换、异常点检测等。常见的RF变种算法如下：Extra Tree、Totally Random Tree Embedding(TRTE)、Isolation Forest

#### 7.1 Extra Tree

Extra Tree是随机森林(RF)的一个变种，原理基本和随机森林一样，区别如下：

1. 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。
2. RF在选择划分特征点的时候会和传统决策树一样，会基于信息增益、信息增益率、基尼系数、均方差等原则来选择最优的特征值。Extra Tree则是随机选择一个特征值来划分决策树。
   由于Extra Tree是随机选择特征点进行划分，所以最后得到的决策树规模会大于RF生成的决策树，Extra Tree决策树的方差会减少，泛化能力比RF更强。

#### 7.2 Totally Random Tree Embedding(TRTE)

TRTE是一种非监督的数据转化方式，将低维的数据集映射到高维，从而让高维的数据更好得用于分类回归模型。
TRTE的算法的转化过程类似于RF算法，建立$T$个决策树来拟合数据。当决策树构建完成之后，数据集里的每个数据在$T$个决策树中叶子节点的位置就固定下来了，将位置信息转换为向量即完成了算法的转换。

比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征$x$划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则$x$映射后的特征编码为$([0,1,0,0,0],[0,0,1,0,0],[0,0,0,0,1])$, 共有15维的高维特征。

样本根据TRTE进行转化特征，最后可能得到更高维的数据，也可能得到更低维的数据，如词袋法中的特征有2000个，做完TRTE后只剩下几百个。经过TRTE转化后的编码可以用于无监督的分类操作，将相似的特征码聚类到一起，最后完成分类的操作。

#### 7.3 Isolation Forest (IForest)

IForest是一种异常点检测算法，使用类似RF的方式来检测异常点。
IForest和RF的区别在于：

1. 在随机采样的过程中，一般只需要少量的数据。
2. 在进行决策树的构建过程中，IForest会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。
3. IForest的划分深度是比较小的，即max_depth较小。

区分原因：目的是异常点检测，所以只要能够区分出异常即可，不需要大量的数据；此外在异常点检测过程中，一般也不需要太大规模的决策树。
下面说说 **IForest**为什么可以做异常点判断。
**对于异常点的判断：**将测试样本$x$拟合到$T$棵决策树上，计算每棵树上该样本的叶子节点的深度$h_t(x)$，从而计算出平均深度$h(x)$；然后就可以使用下列公式计算样本点$x$的异常概率值，$p(x,m)$的取值范围为$[0,1]$，越接近1，说明异常点概率越大。
$$
p(x,m)=2^{-\frac{h(x)}{c(m)}}\\
c(m)=2\ln(m-1)+\xi-2\frac{m-1}{m}
$$
**分析：** 欧拉常数 $\xi≈ 0.57721566490153286060651209$。当样本个数$m$确定的时候，c(m) 是一个定值。所以影响异常点出现的概率$p(x,m)$的值，我们只考虑$h(x)$的取值即可。$h(x)$即每棵树上，该样本的叶子节点深度的平均值。$h(x)$越大说明叶子节点的深度越深，公式右侧$- 2$的指数就越小，则说明$p(x,m)$的值越趋向于0。
**结论：** 叶子节点的深度越深，异常点的概率越小。树平均深度越浅，说明异常值概率越大。

### 8. 优缺点

RF的主要优点有：

1. 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。
2. 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
3. 在训练后，可以给出各个特征对于输出的重要性
4. 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
5. 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
6. 对部分特征缺失不敏感。

RF的主要缺点有：

1. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
2. 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

### 9.sklearn参数

#### 9.1分类

```
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification

>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,
...                              random_state=0)
>>> clf.fit(X, y)  
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=2, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
            oob_score=False, random_state=0, verbose=0, warm_start=False)
>>> print(clf.feature_importances_)
[0.14205973 0.76664038 0.0282433  0.06305659]
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]
```

#### 9.2 回归

```
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.datasets import make_regression

>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = RandomForestRegressor(max_depth=2, random_state=0,
...                              n_estimators=100)
>>> regr.fit(X, y)  
RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,
           max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
           oob_score=False, random_state=0, verbose=0, warm_start=False)
>>> print(regr.feature_importances_)
[0.18146984 0.81473937 0.00145312 0.00233767]
>>> print(regr.predict([[0, 0, 0, 0]]))
[-8.32987858]
```

![img](http://cdn.ziiai.com/o_1ce6b8j401oig4gn7ddcb719p4b.jpg)

### 10.应用场景

数据维度相对低（几十维），同时对准确性有较高要求时。

因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。